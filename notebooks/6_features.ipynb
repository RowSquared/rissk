{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Generate Features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andreas/projects/mlss/venv/lib/python3.10/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'main.yaml': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/Users/andreas/projects/mlss/utils/import_utils.py:130: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['roster_level'] = ''\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "from utils.import_utils import *\n",
    "from hydra import initialize, initialize_config_module, initialize_config_dir, compose\n",
    "from omegaconf import OmegaConf\n",
    "with initialize(config_path='../configuration', version_base='1.1'):\n",
    "    config = compose(config_name='main.yaml')\n",
    "survey_list = SurveyManager(config)\n",
    "dfs_paradata, dfs_questionnaires, dfs_microdata = survey_list.get_dataframes(reload=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-13T09:51:33.478819Z",
     "start_time": "2023-07-13T09:51:01.202409Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Microdata based features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "outputs": [],
   "source": [
    "#group_columns = [col for col in dfs_microdata.columns if col.endswith(\"__id\")]+['survey_name', 'survey_version']\n",
    "item_level_columns = ['interview__id', 'VariableName', 'roster_level']\n",
    "\n",
    "feat_item = dfs_microdata[item_level_columns+['value', 'type', 'IsInteger', 'n_answers', 'answer_sequence']].copy()\n",
    "\n",
    "feat_item['value'].fillna('', inplace=True)\n",
    "\n",
    "text_question_mask = (feat_item['type'] == 'TextQuestion')\n",
    "numeric_question_mask = (feat_item['type'] == 'NumericQuestion') & (feat_item['value'] != '')\n",
    "decimal_question_mask = (feat_item['IsInteger'] == False) & (feat_item['value'] != '')\n",
    "\n",
    "# TODO, should we limit to active questions, interviewer only, etc?\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-13T09:53:13.716403Z",
     "start_time": "2023-07-13T09:53:13.456104Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "outputs": [],
   "source": [
    "# f__string_length, length of string answer, if TextQuestions, empty if not\n",
    "feat_item['f__string_length'] = pd.NA\n",
    "feat_item.loc[text_question_mask, 'f__string_length'] = feat_item.loc[text_question_mask, 'value'].str.len()\n",
    "feat_item['f__string_length']=feat_item['f__string_length'].astype('Int64')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-13T09:53:13.958889Z",
     "start_time": "2023-07-13T09:53:13.887280Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "outputs": [],
   "source": [
    "# f__first_digit, first digit of the response if numeric question, empty if not\n",
    "feat_item['f__first_digit'] = pd.NA\n",
    "feat_item.loc[numeric_question_mask, 'f__first_digit'] = feat_item.loc[numeric_question_mask, 'value'].astype(str).str[0].astype('Int64')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-13T09:53:14.328774Z",
     "start_time": "2023-07-13T09:53:14.279342Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "outputs": [],
   "source": [
    "# f__last_digit, modulus of 10 of the response if numeric question, empty if not\n",
    "feat_item['f__last_digit'] = pd.NA\n",
    "feat_item.loc[numeric_question_mask, 'f__last_digit'] = feat_item.loc[numeric_question_mask, 'value'].astype(int) % 10"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-13T09:53:14.687085Z",
     "start_time": "2023-07-13T09:53:14.662462Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "outputs": [],
   "source": [
    "# f__first_decimal, first decimal digit if numeric question, empty if not\n",
    "feat_item['f__first_decimal'] = pd.NA\n",
    "values = feat_item.loc[decimal_question_mask, 'value'].astype(float)\n",
    "feat_item.loc[decimal_question_mask, 'f__first_decimal'] = np.floor(values * 10) % 10\n",
    "feat_item['f__first_decimal']=feat_item['f__first_decimal'].astype('Int64')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-13T09:53:15.099907Z",
     "start_time": "2023-07-13T09:53:15.040149Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "outputs": [],
   "source": [
    "# f__rel_answer_position, relative position of the selected answer\n",
    "feat_item['f__answer_position'] = pd.NA\n",
    "single_question_mask = (feat_item['type']=='SingleQuestion') & (feat_item['n_answers'] > 2 ) # only questions with more than two answers\n",
    "feat_item.loc[single_question_mask, 'f__answer_position'] = feat_item.loc[single_question_mask].apply(lambda row: round(row['answer_sequence'].index(row['value'])/(row['n_answers']-1),3) if (row['value'] in row['answer_sequence']) and pd.notnull(row['value']) else None, axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-13T09:53:16.389162Z",
     "start_time": "2023-07-13T09:53:15.456252Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# f__answers_selected, number of answers selected in a multi-answer or list question\n",
    "# f__share_selected, share between answers selected, and available answers (only for unlinked questions)\n",
    "\n",
    "def count_elements_or_nan(val): # Function to calculate number of elements in a list or return nan\n",
    "    if isinstance(val, list):\n",
    "        return len(val)\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "multi_list_mask = feat_item['type'].isin(['MultyOptionsQuestion', 'TextListQuestion'])\n",
    "feat_item.loc[multi_list_mask,'f__answers_selected'] = feat_item.loc[multi_list_mask, 'value'].apply(count_elements_or_nan)\n",
    "feat_item['f__share_selected'] = round(feat_item['f__answers_selected'] / feat_item['n_answers'],3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-13T09:53:16.411712Z",
     "start_time": "2023-07-13T09:53:16.388164Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Paradata based features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "outputs": [],
   "source": [
    "# generate df with active events done by interviewer prior to rejection/review\n",
    "\n",
    "vars_needed = ['interview__id', 'order', 'event', 'responsible', 'role', 'tz_offset', 'param', 'answer','roster_level', 'datetime_utc', 'VariableName', 'question_seq', 'type', 'QuestionType',  'survey_name', 'survey_version']\n",
    "df_active = dfs_paradata[vars_needed].copy().sort_values(['interview__id', 'order']).reset_index()\n",
    "# TODO @Gabriele, reset the index after appending in para\n",
    "# TODO, remove hidden questions\n",
    "\n",
    "\n",
    "# streamline missing (empty, NaN) to '', important to identify duplicates in terms of roster below\n",
    "df_active.fillna('', inplace=True)\n",
    "\n",
    "# only keep interviewing events prior to Supervisor/HQ interaction\n",
    "events_split = ['RejectedBySupervisor', 'OpenedBySupervisor', 'OpenedByHQ', 'RejectedByHQ']\n",
    "grouped = df_active.groupby('interview__id')\n",
    "df_active['interviewing'] = False\n",
    "for _, group_df in grouped:\n",
    "    matching_events = group_df['event'].isin(events_split)\n",
    "    if matching_events.any():\n",
    "        first_reject_index = matching_events.idxmax() - 1\n",
    "        min_index = group_df.index.min()\n",
    "        df_active.loc[min_index:first_reject_index, 'interviewing'] = True\n",
    "df_active = df_active[df_active['interviewing']]\n",
    "df_active = df_active.drop(columns=['interviewing'])\n",
    "\n",
    " # only keep active events\n",
    "events_to_keep = ['InterviewCreated', 'AnswerSet', 'Resumed', 'AnswerRemoved', 'CommentSet', 'Restarted']\n",
    "df_active = df_active[df_active['event'].isin(events_to_keep)]\n",
    "\n",
    "# only keep events done by interview (in most cases this should be all, after above filters, just in case supervisor or HQ answered something while interviewer answered on web mode)\n",
    "df_active = df_active[df_active['role']==1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-13T09:53:19.928339Z",
     "start_time": "2023-07-13T09:53:18.325504Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "outputs": [],
   "source": [
    "# f__duration_answer, total time spent to record answers, i.e. sum of all time-intervals from active events ending with the item being AnswerSet or AnswerRemoved\n",
    "# f__duration_comment, total time spent to comment, i.e. sum of all time-intervals from active events ending with the item being CommentSet\n",
    "\n",
    "df_time = df_active.copy()\n",
    "\n",
    "# calculate time difference in seconds\n",
    "df_time['time_difference'] = df_time.groupby('interview__id')['datetime_utc'].diff()\n",
    "df_time['time_difference'] = df_time['time_difference'].dt.total_seconds()\n",
    "\n",
    "# time for answers/comments\n",
    "df_time['f__duration_answer'] = df_time.loc[df_time['event'].isin(['AnswerSet', 'AnswerRemoved']), 'time_difference']\n",
    "df_time['f__duration_comment'] = df_time.loc[df_time['event']=='CommentSet', 'time_difference']\n",
    "\n",
    "# summarize on item level\n",
    "df_time = df_time.groupby(item_level_columns).agg(\n",
    "    f__duration_answer=('f__duration_answer', 'sum'),\n",
    "    f__duration_comment=('f__duration_comment', 'sum')\n",
    "    ).reset_index()\n",
    "\n",
    "# merge into feat_item\n",
    "merged_df = feat_item.merge(df_time, on=item_level_columns, how='left')\n",
    "\n",
    "# Find rows from df_time that didn't have a match\n",
    "unmatched_rows = df_time[~df_time.isin(merged_df)]\n",
    "#merged_df = merged_df[(merged_df['value']!='') & (merged_df['f__duration_answer'].isna())]\n",
    "# TODO: @Gabriele investigate why some do not merge, unless deleted, everything from df_time should have an equivalent in feat_item"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-13T19:36:27.407579Z",
     "start_time": "2023-07-13T19:36:26.757659Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "merged_df[df_time.columns]\n",
    "merged_df[pd.isnull(df_time[columns])]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "outputs": [],
   "source": [
    "# last AnswerSet on item-level\n",
    "df_last = df_active[df_active['event']=='AnswerSet'].groupby(item_level_columns).last()\n",
    "df_last = df_last.sort_values(['interview__id', 'order']).reset_index()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-13T12:10:18.865541Z",
     "start_time": "2023-07-13T12:10:18.478831Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "outputs": [],
   "source": [
    "# f__previous_question, f__previous_answer, f__previous_roster for previous answer set\n",
    "df_last['f__previous_question'] = df_last.groupby('interview__id')['VariableName'].shift(fill_value='')\n",
    "df_last['f__previous_answer'] = df_last.groupby('interview__id')['answer'].shift(fill_value='')\n",
    "df_last['f__previous_roster'] = df_last.groupby('interview__id')['roster_level'].shift(fill_value='')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-13T12:39:25.664568Z",
     "start_time": "2023-07-13T12:39:25.577985Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "outputs": [],
   "source": [
    "# f__half_hour, half-hour interval of last time answered\n",
    "df_last['f__half_hour'] = df_last['datetime_utc'].dt.hour + df_last['datetime_utc'].dt.round('30min').dt.minute / 60\n",
    "\n",
    "# f__in_working_hours, indication if f__half_hour is within working hours\n",
    "half_hour_counts = df_last['f__half_hour'].value_counts().sort_index()\n",
    "\n",
    "threshold = half_hour_counts.median()*0.33  # approach 1: interval < 1/3 of the median count of answers set\n",
    "working_hours_1 = half_hour_counts[half_hour_counts >= threshold].index.tolist()\n",
    "\n",
    "cumulative_share = (half_hour_counts.sort_values().cumsum()/half_hour_counts.sum()).sort_index()\n",
    "working_hours_2 = half_hour_counts[cumulative_share >= 0.05].index.tolist() # approach 2: the least frequent intervals with total of 5% of answers set\n",
    "\n",
    "df_last['f__in_working_hours'] = df_last['f__half_hour'].isin(working_hours_2)\n",
    "\n",
    "# TODO: add timezone offset, think about if we want to do this by day of the week or by calendar day?\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-13T13:13:23.330423Z",
     "start_time": "2023-07-13T13:13:23.176411Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "outputs": [],
   "source": [
    "# sequence\n",
    "df_last['sequential'] = df_last.groupby('interview__id').cumcount() + 1\n",
    "\n",
    "df_last['diff'] = df_last['sequential']  - df_last['question_seq']\n",
    "df_last['seq_jump'] = df_last['diff'].diff()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-13T13:51:55.404244Z",
     "start_time": "2023-07-13T13:51:55.327395Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
