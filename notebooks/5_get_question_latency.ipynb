{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Question latency\n",
    "\n",
    "\n",
    "1. Calculate the response latency for each question, i.e. the time in seconds from the previous questions being answered/commented to the current question being answered/commented.\n",
    "2. Considers only the timing for questions answered by interviewers prior to any supervisor/HQ rejection/review event.\n",
    "3. It calculates the total time spent for one question/roster-level, and counts the number of time the question was visited (answer set or commented, ignoring consecutive events for one question/roster-level).\n",
    "5. We need to move over the fillna part to the paradata generation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andreas/projects/mlss/venv/lib/python3.10/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'main.yaml': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "from utils.import_utils import *\n",
    "from hydra import initialize, initialize_config_module, initialize_config_dir, compose\n",
    "from omegaconf import OmegaConf\n",
    "with initialize(config_path='../configuration', version_base='1.1'):\n",
    "    config = compose(config_name='main.yaml')\n",
    "survey_list = SurveyManager(config)\n",
    "dfs_paradata, dfs_questionnaires, dfs_microdata = survey_list.get_dataframes(reload=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T11:20:07.065648Z",
     "start_time": "2023-07-07T11:19:25.358108Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# generate new df\n",
    "vars_needed = ['interview__id', 'order', 'event', 'responsible', 'role', 'tz_offset', 'param', 'answer','roster_level', 'datetime_utc', 'VariableName', 'question_seq', 'type', 'QuestionType']\n",
    "df_time = dfs_paradata[vars_needed].copy()\n",
    "\n",
    "# streamline missings (empty, NaN) to '', important to identify duplicates in terms of roster below\n",
    "df_time.fillna('', inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T11:55:12.644586Z",
     "start_time": "2023-07-07T11:55:12.117437Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# only keep  interviewing events prior to Supervisor/HQ interaction\n",
    "events_split = ['RejectedBySupervisor', 'OpenedBySupervisor', 'OpenedByHQ', 'RejectedByHQ']\n",
    "grouped = df_time.groupby('interview__id')\n",
    "df_time['interviewing'] = False\n",
    "for _, group_df in grouped:\n",
    "    first_reject_index = group_df['event'].isin(events_split).idxmax()-1\n",
    "    min_index = group_df.index.min()\n",
    "    df_time.loc[min_index:first_reject_index, 'interviewing'] = True\n",
    "df_time = df_time[df_time['interviewing']]\n",
    "df_time = df_time.drop(columns=['interviewing'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    " # keep only events relevant for calculating response latency\n",
    "\n",
    "#events_to_drop = ['SupervisorAssigned', 'InterviewerAssigned', 'KeyAssigned', 'VariableDisabled','ReceivedByInterviewer', 'KeyAssigned', 'VariableEnabled', 'VariableSet', 'QuestionDeclaredInvalid', 'QuestionDeclaredValid', 'Completed', 'TranslationSwitched','ReceivedBySupervisor','OpenedBySupervisor','ApproveBySupervisor','ClosedBySupervisor', 'InterviewModeChanged', 'Paused', 'RejectedBySupervisor']\n",
    "\n",
    "events_to_keep = ['InterviewCreated', 'AnswerSet', 'Resumed', 'AnswerRemoved', 'CommentSet', 'Restarted'] # check in other example data sets that there are no other relevant events\n",
    "df_time = df_time[df_time['event'].isin(events_to_keep)]\n",
    "\n",
    "#df_time = df_time[~df_time['event'].isin(events_to_drop)] # to x-check we have all interviewer events\n",
    "#df_time['event'].unique()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T11:55:14.853332Z",
     "start_time": "2023-07-07T11:55:14.753396Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# keep only events done by interview (should not exist for most cases after above filters, just in case supervisor or HQ answered something while interviewer answered on web mode)\n",
    "df_time = df_time[df_time['role']==1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T11:55:17.422340Z",
     "start_time": "2023-07-07T11:55:17.317412Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "# if the same question was repeatedly answered/commented on the same roster level, keep only the last one (to take the overall time for the question)\n",
    "group_col = ['interview__id', 'VariableName', 'roster_level']\n",
    "df_time['is_diff'] = (df_time[group_col].shift() != df_time[group_col]).any(axis=1)\n",
    "df_time['keep'] = df_time['is_diff'].shift(-1, fill_value=True)\n",
    "df_time = df_time[df_time['keep']]\n",
    "df_time.drop(columns=['is_diff', 'keep'], inplace=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T11:57:46.857989Z",
     "start_time": "2023-07-07T11:57:46.634279Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "# calculate time difference in seconds\n",
    "df_time['time_difference'] = df_time.groupby('interview__id')['datetime_utc'].diff()\n",
    "df_time['time_difference'] = df_time['time_difference'].dt.total_seconds()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T11:57:53.853517Z",
     "start_time": "2023-07-07T11:57:53.808967Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "# keep only AnswerSet and CommentSet events, we ignore timing for AnswerRemoved as it is also system generated\n",
    "df_time = df_time[df_time['event'].isin(['AnswerSet', 'CommentSet'])]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T12:15:07.923266Z",
     "start_time": "2023-07-07T12:15:07.801720Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "# sum total time per question and roster level, count number of times the question was revisited (answered or commented, after other questions were answered)\n",
    "\n",
    "df_latency = df_time.groupby(group_col).agg(\n",
    "    total_duration=('time_difference', 'sum'),\n",
    "    n_revisited=('time_difference', 'count')\n",
    "    ).reset_index()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T12:26:57.885624Z",
     "start_time": "2023-07-07T12:26:57.750934Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
