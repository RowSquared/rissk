<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>What is RISSK? &mdash; RISSK 0.0.1 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="_static/jquery.js?v=5d32c60e"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js?v=f6245a2f"></script>
        <script src="_static/doctools.js?v=888ff710"></script>
        <script src="_static/sphinx_highlight.js?v=4825356b"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="#" class="icon icon-home">
            RISSK
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <!-- Local TOC -->
              <div class="local-toc"><ul>
<li><a class="reference internal" href="#">What is RISSK?</a></li>
<li><a class="reference internal" href="#getting-started">Getting started</a><ul>
<li><a class="reference internal" href="#prerequisites">Prerequisites</a></li>
<li><a class="reference internal" href="#setup">Setup</a></li>
<li><a class="reference internal" href="#running-the-package">Running the Package</a></li>
</ul>
</li>
<li><a class="reference internal" href="#advanced-use">Advanced Use</a><ul>
<li><a class="reference internal" href="#export-score-and-feature-files">Export score and feature files</a></li>
<li><a class="reference internal" href="#exclude-features">Exclude features</a></li>
<li><a class="reference internal" href="#set-contamination-level">Set contamination level</a></li>
</ul>
</li>
<li><a class="reference internal" href="#interpretation">Interpretation</a></li>
<li><a class="reference internal" href="#survey-integration">Survey integration</a></li>
<li><a class="reference internal" href="#limitations">Limitations</a></li>
<li><a class="reference internal" href="#confirmation-of-results">Confirmation of results</a><ul>
<li><a class="reference internal" href="#testing">Testing</a></li>
<li><a class="reference internal" href="#experiment">Experiment</a></li>
</ul>
</li>
<li><a class="reference internal" href="#process-description">Process description</a></li>
<li><a class="reference internal" href="#roadmap">Roadmap</a></li>
</ul>
</div>
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="#">RISSK</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="#" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">What is RISSK?</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/index.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <p><img alt="RISSK Logo" src="https://github.com/RowSquared/mlss/blob/main/rissk.png" /></p>
<section id="what-is-rissk">
<h1>What is RISSK?<a class="headerlink" href="#what-is-rissk" title="Permalink to this heading"></a></h1>
<p>RISSK creates an easy-to-interpret <strong>Unit Risk Score (URS)</strong> directly from your <strong><a class="reference external" href="https://mysurvey.solutions/en/">Survey Solutions</a></strong> export files. The score indicates how much individual interviews are at-risk of including undesired interviewer behaviour such as data fabrication and can be used to target suspicious interviews during verification exercises such as back-check interviews. RISSK is generic and can easily be integrated into the monitoring system of most CAPI or CATI surveys run in Survey Solutions. It works by extracting a range of generic features from the microdata and paradata exports, identifying anomalies and combining individual scores into the URS using Principal Component Analysis.</p>
</section>
<section id="getting-started">
<h1>Getting started<a class="headerlink" href="#getting-started" title="Permalink to this heading"></a></h1>
<p>These instructions will guide you on how to install and run RISSK on your local machine.</p>
<section id="prerequisites">
<h2>Prerequisites<a class="headerlink" href="#prerequisites" title="Permalink to this heading"></a></h2>
<p>Make sure you have Python 3.8 or higher installed on your machine. You can verify this by running:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>--version
</pre></div>
</div>
</section>
<section id="setup">
<h2>Setup<a class="headerlink" href="#setup" title="Permalink to this heading"></a></h2>
<ol class="arabic simple">
<li><p>(Optional) When setting up MLSS, a project folder <code class="docutils literal notranslate"><span class="pre">mlss</span></code> will be created on your local machine. Navigate to the directory where you would like the <code class="docutils literal notranslate"><span class="pre">mlss</span></code> folder to be created. If not done, the directory <code class="docutils literal notranslate"><span class="pre">mlss</span></code> will be created in your currently active working directory. For example, to create the <code class="docutils literal notranslate"><span class="pre">mlss</span></code> directory in <code class="docutils literal notranslate"><span class="pre">users/USER/projects</span></code>:</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cd</span> <span class="o">/</span><span class="n">Users</span><span class="o">/</span><span class="n">USER</span><span class="o">/</span><span class="n">projects</span>
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p>Clone this repository to your local machine.</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">git</span> <span class="n">clone</span> <span class="n">git</span><span class="nd">@github</span><span class="o">.</span><span class="n">com</span><span class="p">:</span><span class="n">RowSquared</span><span class="o">/</span><span class="n">mlss</span><span class="o">.</span><span class="n">git</span>
</pre></div>
</div>
<ol class="arabic simple" start="3">
<li><p>Navigate into the project directory <code class="docutils literal notranslate"><span class="pre">mlss</span></code>.</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cd</span> <span class="n">mlss</span>
</pre></div>
</div>
<ol class="arabic simple" start="4">
<li><p>Create a new virtual environment inside the project directory.</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">venv</span> <span class="n">venv</span>
</pre></div>
</div>
<ol class="arabic simple" start="5">
<li><p>Activate the virtual environment. The command to do this will depend on your operating system:
On <strong>Windows</strong>:</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">venv</span>\<span class="n">Scripts</span>\<span class="n">activate</span>
</pre></div>
</div>
<p>On <strong>Unix or MacOS</strong>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">source</span> <span class="n">venv</span><span class="o">/</span><span class="nb">bin</span><span class="o">/</span><span class="n">activate</span>
</pre></div>
</div>
<ol class="arabic simple" start="6">
<li><p>Install the required dependencies:</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">r</span> <span class="n">requirements</span><span class="o">.</span><span class="n">txt</span>
</pre></div>
</div>
</section>
<section id="running-the-package">
<h2>Running the Package<a class="headerlink" href="#running-the-package" title="Permalink to this heading"></a></h2>
<ol class="arabic simple">
<li><p>Export from Survey Solutions the Main Survey Data and Paradata for all versions of one questionnaire (survey template). The Main Survey Data must be either in <strong>Tab separated</strong> or <strong>Stata 14</strong> format, and must <strong>Include meta information about questionnaire</strong>.</p></li>
<li><p>Place the exported zip files into a folder. The path to this folder is referred to below as <em>&lt;export_path&gt;</em>. Do not modify the zip files. The folder must contain one zip file for Paradata and one zip file for Main Survey Data for each version of the questionnaire that should be analysed. To exclude versions, do not add their files</p></li>
<li><p>(If running after the installation), navigate to the <code class="docutils literal notranslate"><span class="pre">mlss</span></code> directory.</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cd</span> <span class="o">/</span><span class="n">Users</span><span class="o">/</span><span class="n">USER</span><span class="o">/</span><span class="n">projects</span><span class="o">/</span><span class="n">mlss</span>
</pre></div>
</div>
<ol class="arabic simple" start="4">
<li><p>Run the package, using below command and replacing</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">&lt;export_path&gt;</span></code> with the string of the path to the directory containing your Survey Solutions export data.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&lt;output_file&gt;</span></code> with the string of the file path of the name of the output csv file. Note, it must include the path, not just the name. If not specified, results will be stored in directory <code class="docutils literal notranslate"><span class="pre">rissk/result/unit_risk_score.csv</span></code>.</p></li>
</ul>
</li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">main</span><span class="o">.</span><span class="n">py</span> <span class="n">export_path</span><span class="o">=&lt;</span><span class="n">export_path</span><span class="o">&gt;</span> <span class="n">output_file</span><span class="o">=&lt;</span><span class="n">output_file</span><span class="o">&gt;</span>
</pre></div>
</div>
<ol class="arabic simple" start="5">
<li><p>After executing the command, you should see console logs indicating the progress of the package. Upon successful completion, a message will confirm that the results have been saved to the specified directory. They INCLUDE XYZ.</p></li>
</ol>
</section>
</section>
<section id="advanced-use">
<h1>Advanced Use<a class="headerlink" href="#advanced-use" title="Permalink to this heading"></a></h1>
<p>This chapter provides additional information for users who would like to dig deeper and adjust or expand the functioning of the package.</p>
<section id="export-score-and-feature-files">
<h2>Export score and feature files<a class="headerlink" href="#export-score-and-feature-files" title="Permalink to this heading"></a></h2>
<p>Scores files can be exported</p>
</section>
<section id="exclude-features">
<h2>Exclude features<a class="headerlink" href="#exclude-features" title="Permalink to this heading"></a></h2>
<p>By default, all features are included in the construction of the URS. Users can exclude individual features if they are affecting the score in undesired ways. This may be the case, e.g., if a feature has been observed to drive the URS for some interviews, but external validation have revealed those interviews to be of low-risk.</p>
<p>To turn off a feature and exclude it from the Unit Risk Score, open
<code class="docutils literal notranslate"><span class="pre">environment/main.yaml</span></code> and change the <code class="docutils literal notranslate"><span class="pre">use:</span></code> property to <code class="docutils literal notranslate"><span class="pre">false</span></code> for the respective feature prior to rerunning the package.
For example, below, the feature<code class="docutils literal notranslate"><span class="pre">answer_changed</span></code> has been excluded from the URS.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">features</span><span class="p">:</span>
<span class="w">  </span><span class="nt">answer_time_set</span><span class="p">:</span>
<span class="w">    </span><span class="nt">use</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">  </span><span class="nt">answer_changed</span><span class="p">:</span>
<span class="w">    </span><span class="nt">use</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
</pre></div>
</div>
</section>
<section id="set-contamination-level">
<h2>Set contamination level<a class="headerlink" href="#set-contamination-level" title="Permalink to this heading"></a></h2>
<p>The algorithms used in the calculation of some of the scores require a contamination level to be set. By default, RISSK uses the <code class="docutils literal notranslate"><span class="pre">medfilt</span></code> thresholding method to automatically determine the contamination level. This can be overwritten by specifying the <code class="docutils literal notranslate"><span class="pre">contamination</span></code> parameter for the specific score in the <code class="docutils literal notranslate"><span class="pre">environment/main.yaml</span></code>. In below example, a contamination level of 0.1 will be used for the ECOD algorithm in <code class="docutils literal notranslate"><span class="pre">answer_changed</span></code>.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="w">  </span><span class="nt">answer_changed</span><span class="p">:</span>
<span class="w">    </span><span class="nt">use</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">    </span><span class="nt">parameters</span><span class="p">:</span>
<span class="w">      </span><span class="nt">contamination</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.1</span>
</pre></div>
</div>
<p>Refer to <a class="reference internal" href="#FEATURES_SCORES.md"><span class="xref myst">FEATURES_SCORES.md</span></a> to learn for which features the <code class="docutils literal notranslate"><span class="pre">contamination</span></code> parameter can be set.</p>
</section>
</section>
<section id="interpretation">
<h1>Interpretation<a class="headerlink" href="#interpretation" title="Permalink to this heading"></a></h1>
<p>MLSS produces <code class="docutils literal notranslate"><span class="pre">FILE</span></code> in folder <code class="docutils literal notranslate"><span class="pre">&lt;result_path&gt;</span></code> containing the URS for each interview.</p>
<p>Variable <code class="docutils literal notranslate"><span class="pre">unit_risk_score</span></code> contains the URS. It ranges from value 0 for the interview(s) with the lowest risk to value 100 for interview(s) with the highest risk. The higher the risk score, the more likely it is for an interview file to contain problematic interviewer behaviour, such as data fabrication. To identify such behaviour, RISSK searches for anomalies in the following features:</p>
<ul class="simple">
<li><p>hours of the day during which the interview was conducted</p></li>
<li><p>duration of the interview and of individual questions</p></li>
<li><p>locations (if any GPS questions are set)</p></li>
<li><p>question sequence followed in the interview</p></li>
<li><p>how answers were changed or removed</p></li>
<li><p>duration of pauses in the interview</p></li>
<li><p>the position and share of answers selected</p></li>
<li><p>the variance and entropy of answers</p></li>
<li><p>the distribution of digits of answers</p></li>
<li><p>number of questions answered and unanswered</p></li>
</ul>
<p>The higher the URS, the more unusual an interview is in the above features, and the more an interview should be prioritized for verification and review. To find out which features and score drive the <code class="docutils literal notranslate"><span class="pre">unit_risk_score</span></code>, use the optional export, see chapter <a class="reference internal" href="#export-score-and-feature-files"><span class="xref myst">Export score and feature files</span></a>. Refer to chapter <a class="reference internal" href="#process-description"><span class="xref myst">Process description</span></a> for description of how the URS is constructed, and to <a class="reference internal" href="#FEATURES_SCORES.md"><span class="xref myst">FEATURS_SCORES.md</span></a> for details on all features and scores.</p>
<!-- Gabriele, how does this work? -->
<blockquote>
<div><p>[!WARNING]
The URS is <strong>not</strong> proof of interviewer misbehaviour. The score includes <strong>false positives</strong> (legitimate interviews with high URS), e.g., if unusual circumstances cause a high score for a valid interview. It also includes <strong>false negatives</strong> (problematic interviews with low URS) if algorithms do not detect anomalies in some of the problematic interviews. To prove interviewer misbehaviour, further evidence or investigation is needed, see  chapter <a class="reference internal" href="#survey-integration"><span class="xref myst">Survey integration</span></a>.</p>
</div></blockquote>
<!--equal distance between scores ?-->
<p>The URS is a <em>relative</em> measure and depends on the patterns in the interviews in the Survey Solutions export files. If RISSK is executed again with new export files, previously obtained <code class="docutils literal notranslate"><span class="pre">unit_risk_score</span></code> will change for the same interview, since patterns in the export filed will have changed. When comparing the URS between interviews, it is important to use scores that were created using the same export files. The relative nature of the URS also implies that <code class="docutils literal notranslate"><span class="pre">unit_risk_score</span></code> cannot be compared directly between surveys.</p>
<p>By design, RISSK only considers those parts of the interviews that happened prior to the first interaction by a Supervisor or HQ role with an interview file. This is done to exclude post-interview interviewer actions that would otherwise create confounding patterns. If significant parts of an interview were completed after this interaction, these parts will not be considered, and the URS can be off. This also implies that URS of an interview cannot be improved by rejecting an interview and modifying it.</p>
<p>Please note the RISSK does not (yet) consider outstanding error messages and interviewer comments set. These are easily accessible in the Survey Solution interface and are ideally reviewed systematically.</p>
<!-- something on the distribution of the scores, maybe test results? 
For our test data, unit_risk_scare was a positively scewed distribution, as is common in fraud detection. ---> 
</section>
<section id="survey-integration">
<h1>Survey integration<a class="headerlink" href="#survey-integration" title="Permalink to this heading"></a></h1>
<p>Use RISSK to prioritize the most at-risk interviews in your quality assurance processes (in-depth review, back-checks, audio auditing, etc.), so more issues can be detected more easily. Additionally, the URS can be monitored as an indicator to identify trends by interviewer and over time. This chapter lines out how this can be achieved generally. The best way of integrating RISSK into a survey depends on the specific context and resources available. For specific advice, please contact the authors.</p>
<blockquote>
<div><p>[!WARNING]<br />
RISSK is intended to inform and complement other components of a data quality assurance system, such as back-checks, audio audits or indicator monitoring. It does NOT make them redundant.</p>
</div></blockquote>
<p>Ideally, RISSK is run (and the results reviewed and acted upon) on a regular basis throughout fieldwork, so issues can be detected and resolved as early as possible. For most surveys, somewhere between <strong>daily</strong> and <strong>weekly</strong> should be a good frequency. Usually this implies that the output of RISSK needs to be digested and worked through in batches.</p>
<p>If existing, it is desirable to integrate the package into the survey’s data management and monitoring system, so all available monitoring information is combined and executing the package and handling the output is automated. An example set-up can be:</p>
<ul class="simple">
<li><p>Execute RISSK as part of the scripts that export from Survey Solutions.</p></li>
<li><p>Using the output, identify the interviews to be verified/reviewed and add them to the backlog for supervisors or data monitors.</p></li>
<li><p>Use the output to summarize the URS together with other indicators in the monitoring dashboard.</p></li>
</ul>
<p>The URS is designed to inform the <strong>first</strong> review/verification of an interview file, as it only considers what was answered before a Supervisor or HQ role opened or rejected the file (see chapter <a class="reference internal" href="#interpretation"><span class="xref myst">Interpretation</span></a>). Interviewer actions that happened after the first rejection must be monitored otherwise.</p>
<p>For any given batch, it is most efficient to prioritize the interviews with the highest URS for review/verification, as these are most likely to be problematic. In our testing survey, of the 10% interviews with the highest URS, 67% were fabricated. Since the URS also contains false negatives (problematic interviews with lower scores), it may also be desirable to review/verify some interviews with lower URS. As an example, one strategy could be to review/verify for every batch 10% of the interviews with the highest URS, and another 5% selected from the rest (e.g., random, highest URS by interviewer, etc..).</p>
<p>The review/verification of prioritized interviews can include activities such as:</p>
<ul class="simple">
<li><p>Ideally, an external verification of the interview by either:</p>
<ul>
<li><p>conducting a short back check interview (to establish if interview happened and verify key questions),</p></li>
<li><p>auditing the audio recording of the interview (most insightful about interviewer behaviour).</p></li>
</ul>
</li>
<li><p>In-depth review if the interview and the paradata (though often inconclusive).</p></li>
<li><p>Interviewer queries or confrontation, e.g,. <em>“Our monitoring system has flagged the interview you did yesterday as very suspicious. Is there something you want to tell me about it?”</em>. (limited)</p></li>
<li><p>Additional interview observations (only effective for unintentional interviewer wrongdoings)</p></li>
</ul>
<p>Keep a structured record of the outcome of the review/verification, i.e. if individual interviews were found to contain problematic behaviour, and if so of what nature. You can use this information to finetune the composition of the URS (see chapter <a class="reference internal" href="#advanced-use"><span class="xref myst">Advanced use</span></a>). The authors would also welcome such outcome, together with the result files to further improve RISSK.</p>
<p>If interviewer misbehaviour could be verified, clear consequences should follow, e.g., stern warning (<em>“yellow card”</em>), loss of bonus, retraining, dismissal. It may also be useful to review/verify other interviews by the same interviewer (guided by the URS), and re-interview all affected respondents if necessary.</p>
<p>It is usually beneficial to let interviewers know that they are being monitored and that an algorithm is used to identify suspicious interviews. This helps them to do well (people do better if they know it matters what they do) and is a deterrent from doing bad (there is real chance of being caught and there are be consequences).</p>
<p>However, in your feedback to the field teams, do <strong>NOT</strong> reveal details of the algorithm, such as what features are being checked or the scores that are driving the URS of an interview. Interviewers may start to learn what behaviour to avoid or circumvent, reducing RISSK’s ability to identify problematic interviews. As an example, if you are providing feedback such as <em>“We are checking the hour of the day and I see that you have done this interview at night!”</em>, the interviewer may start to fabricate interviews during the day instead or change the tablet time. Instead, provide generic feedback <em>“Your interview has been flagged.”</em>, ask the interviewers to provide you with details about the interview <em>“When did you do it? With who? How many visits did you need? Any specific things to note?”</em> and see if the story matches the paradata.</p>
<p>To use the URS as a monitoring indicator, average <code class="docutils literal notranslate"><span class="pre">unit_risk_score</span></code> by interviewer (and/or team) and over time (week/month), and visualize it e.g. as part of a survey monitoring dashboard. While individual interviews by one interviewer may not score high enough to be reviewed/verified, a repeated high average score over time for one interviewer may signal potential issues and the need to take action. Monitoring the average URS by interviewer and time also helps to check if interviewers have adjusted to feedback or warnings (lower URS post-intervention) or continue to produce problematic interviews (equal or higher URS).</p>
<blockquote>
<div><p>[!IMPORTANT]
If your survey uses multiple questionnaires, such as separate household and community questionnaires, RISSK must be executed separately for each template.</p>
</div></blockquote>
</section>
<section id="limitations">
<h1>Limitations<a class="headerlink" href="#limitations" title="Permalink to this heading"></a></h1>
<ul class="simple">
<li><p>MLSS assumes that the majority of interviews are conducted as desired, which determines normal behaviour. The scores may break down for surveys with extreme levels of problematic interviewer behaviour.</p></li>
<li><p>With low number of interviews (e.g., during the first few days of fieldwork) the scores are less effective and reliable.</p></li>
<li><p>MLSS is not reliable for interviews that were in a significant part filled in after the first supervisor or HQ related events in the paradata, e.g. if an interview file was originally submitted almost empty and later rejected to be completed by the interviewer. By design, MLSS only considers the part of the interview that happened prior to the first interaction of a Supervisor or HQ role with the interview file.</p></li>
<li></li>
<li><p>If you use <a class="reference external" href="https://docs.mysurvey.solutions/headquarters/config/admin-settings/">partial synchronization</a> and would like to use MLSS, Supervisor and HQ roles should not open interview files prior to their completion.</p></li>
<li><p>MLSS has been tested on laptop with XYZ GB of RAM, for paradata up to x M rows, or a survey of XX questions and YY interviews. To process very large datasets may require a server with higher memory.</p></li>
<li><p>The tool does not (yet) accept microdata exports from Survey Solutions in the SPSS format. Export to STATA or TAB instead.</p></li>
<li><p>Interviews containing non-contact or non-response cases may distort the URS, as they often follow a different (much shorter) path in the questionnaire. <!-- @Gabriele, we need to test this --></p></li>
<li><p>For barcode, picture, audio and geography questions, no microdata based features have been developed. These questions are only considered through their related events in the paradata.</p></li>
<li><p>The tool has been conceptualized for CAPI or CATI interviews. It has not been tested for surveys run in Survey Solution’s CAWI mode.</p></li>
</ul>
</section>
<section id="confirmation-of-results">
<h1>Confirmation of results<a class="headerlink" href="#confirmation-of-results" title="Permalink to this heading"></a></h1>
<section id="testing">
<h2>Testing<a class="headerlink" href="#testing" title="Permalink to this heading"></a></h2>
<p>confirmation on tests (in our tests we observed that x moving up, moved up the score by … )</p>
<!-- Going to do this or should I delete -->
</section>
<section id="experiment">
<h2>Experiment<a class="headerlink" href="#experiment" title="Permalink to this heading"></a></h2>
<p>To verify the feature generation, anomaly detection and scoring system, we required survey data with reliable interview-level quality labels. We infused a real CATI survey (name and details cannot be given due to a non-disclosure agreement) with artificial high-at-risk interviews, by asking interviewers to produce fake interviews just after the completion of the real survey. 7 different scenarios were given to induce variation. For some scenarios, interviewers were incentivized to give their best. 11 interviewers created 1 interview file for each of the following scenarios in sequence, or a total of 77 fake interviews.</p>
<ol class="arabic simple">
<li><p>Non-incentivized. Pretend you are interviewing and fill in the questionnaire.</p></li>
<li><p>Incentivized. Fake as good as you can, try not to get caught.
Same as Scenario 2.</p></li>
<li><p>Incentivized. Fake as good as you can, try to be realistic in timings.</p></li>
<li><p>Incentivized. Fake as good as you can, try to set as real answers as possible.</p></li>
<li><p>Non-incentivized. Fake without putting effort.</p></li>
<li><p>Incentivized. Fake as fast as possible.</p></li>
</ol>
<p>The artificial fake interviews were combined with all 268 real interviews from the survey to form our testing data set, including 345 interviews in total. Real interviews for this survey are believed to be of general low-risk, as they were conducted by a small team of interviewers with a trusted, long-term relationship, incentives to perform well and deterrents to do badly, as well as a good data monitoring structure in place. Furthermore, interviewers were aware that the data they collected would be used to validate secondary data and that discrepancies would be investigated. Nevertheless, it could not be ruled out that some real interviews contained problematic interviewer behaviour.</p>
<p>When using <code class="docutils literal notranslate"><span class="pre">unit_risk_score</span></code> to cluster interviews into real and fake, we achieve a <a class="reference external" href="https://en.wikipedia.org/wiki/Precision_and_recall">recall</a>/<a class="reference external" href="https://en.wikipedia.org/wiki/Sensitivity_and_specificity">sensitivity</a> of 61 %, i.e. of the artificially created fakes, 61 % were correctly classified. To measure the effectiveness in practical survey setting, we sort the interviews by <code class="docutils literal notranslate"><span class="pre">unit_risk_score</span></code> and select the top <em>N</em> percent of interviews, as one would do when using the URS to prioritize interviews for review/verification. We then calculate <code class="docutils literal notranslate"><span class="pre">share_urs</span></code>, the share of fakes among the selected interviews in percent and compare it to <code class="docutils literal notranslate"><span class="pre">share_rand</span></code>, the share of fakes one would obtain if selecting <em>N</em> percent of interviews at random, which is equal to the prevalence of fakes in the data. The table below summarizes the results for the top 5, 10, 15 and 20 percent.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-right"><p>N</p></th>
<th class="head text-right"><p>share_urs</p></th>
<th class="head text-right"><p>share_rand</p></th>
<th class="head text-right"><p>share_urs/share_rand</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-right"><p>5%</p></td>
<td class="text-right"><p>66.1%</p></td>
<td class="text-right"><p>22.4%</p></td>
<td class="text-right"><p>2.8</p></td>
</tr>
<tr class="row-odd"><td class="text-right"><p>10%</p></td>
<td class="text-right"><p>x</p></td>
<td class="text-right"><p>22.4%</p></td>
<td class="text-right"><p>x</p></td>
</tr>
<tr class="row-even"><td class="text-right"><p>15%</p></td>
<td class="text-right"><p>y</p></td>
<td class="text-right"><p>22.4%</p></td>
<td class="text-right"><p>y</p></td>
</tr>
<tr class="row-odd"><td class="text-right"><p>20%</p></td>
<td class="text-right"><p>z</p></td>
<td class="text-right"><p>22.4%</p></td>
<td class="text-right"><p>z</p></td>
</tr>
</tbody>
</table>
<p>In our experiment, selecting the top 5% of interviews with the highest URS, would contain xyx % of fake interviews, which is 2.8 times higher than selecting interviews it at random. As more interviews are selected, <code class="docutils literal notranslate"><span class="pre">share_urs</span></code> decreases. If selecting 20% of the interviews based on the URS, ZZZ % will be fake, which is still 1.4. higher than selecting at random.</p>
<p>Below chart summarizes how <code class="docutils literal notranslate"><span class="pre">share_urs</span></code> behaves as we increase the number of interviews selected continuously from 1 to 100 of all interviews. The horizontal line at 22.4% equals <code class="docutils literal notranslate"><span class="pre">share_rand</span></code>.</p>
<!-- add chart -->
<p>Please note that these results only take into account the classification of interviews into real/fake assigned from the experiment groups. While none of the fake interviews can be without issues, some of the real interviews with relatively high <code class="docutils literal notranslate"><span class="pre">unit_risk_score</span></code> may include problematic behaviour, which would increase <code class="docutils literal notranslate"><span class="pre">share_urs</span></code>.</p>
<blockquote>
<div><p>[!NOTE]
The effectiveness is likely to differ between surveys as it depends on the nature of problematic interviews.</p>
</div></blockquote>
</section>
</section>
<section id="process-description">
<h1>Process description<a class="headerlink" href="#process-description" title="Permalink to this heading"></a></h1>
<p>How does it work. The</p>
<p>The package extracts a range of features from the microdata and paradata and identifies anomalies. Individual scores are combined into the URS ranging from 0 to 100.</p>
<p>This chapter describes in broad terms the individual steps of the package.</p>
<p><strong>Data preparation</strong></p>
<ol class="arabic simple">
<li><p><strong>Unzip</strong>. The tool looks within &lt;survey_folder_path&gt; for Survey Solutions export files in the Paradata and STATA or Tabular format that match the questionnaire  &lt;survey_name&gt; . For each version, export files are unzipped to a respective subfolder within <code class="docutils literal notranslate"><span class="pre">mlss/data/raw/&lt;survey_name&gt;</span></code>. Within each subfolder, the file <code class="docutils literal notranslate"><span class="pre">Questionnaire/content.zip</span></code> is unzipped.</p></li>
<li><p><strong>Build questionnaire data</strong>. For each version, the tool constructs a dataframe <code class="docutils literal notranslate"><span class="pre">df_questionnaire</span></code> using the questionnaire json file <code class="docutils literal notranslate"><span class="pre">Questionnaire/content/document.json</span></code> and the Excel files contained in <code class="docutils literal notranslate"><span class="pre">Questionnaire/content/Categories</span></code>. The dataframe contains one row for every questionnaire item (e.g., questions, variables, sub-sections, etc.) of the Survey Solutions questionnaire, and columns corresponding to (most of) the item properties (e.g.,  question type, variable name, etc.).</p></li>
<li><p><strong>Build microdata</strong>. For each version, the tool identifies all export files within the subfolder containing microdata, i.e. all files whose name does not start with ‘interview__’ or  ‘assignment__’. For each file:</p>
<ul class="simple">
<li><p>The export file is loaded as a dataframe.</p></li>
<li><p>(If loaded from STATA), non-response values are adjusted to match the Tabular export format.</p></li>
<li><p>Columns relating to questions containing multiple variables in export files are transformed to a single column (multi-select and list questions to lists, GPS questions to string).</p></li>
<li><p>System-generated variables are dropped, i.e.,<code class="docutils literal notranslate"><span class="pre">['interview__key',</span> <span class="pre">'sssys_irnd',</span> <span class="pre">'has__errors',</span> <span class="pre">'interview__status',</span> <span class="pre">'assignment__id']</span></code>.</p></li>
<li><p>The dataframe is reshaped to long (melt) and all dataframes for one version are appended. The resulting dataframe contains the columns <code class="docutils literal notranslate"><span class="pre">['interview__id',</span> <span class="pre">'roster_level',</span> <span class="pre">'variable_name',</span> <span class="pre">'value']</span></code>.</p></li>
<li><p>All rows relating to disabled questions or Survey Solution variables are dropped.</p></li>
<li><p>Question properties are merged in from <code class="docutils literal notranslate"><span class="pre">df_questionnaire</span></code>.</p></li>
<li><p>The resulting dataframe df_microdata contains all microdata that could have been set by the interviewer (or preloaded).</p></li>
</ul>
</li>
<li><p><strong>Build paradata</strong>: For each version, the paradata file is loaded as dataframe. The column <code class="docutils literal notranslate"><span class="pre">parameters</span></code> is split into <code class="docutils literal notranslate"><span class="pre">param</span></code>, <code class="docutils literal notranslate"><span class="pre">roster_level</span></code> and <code class="docutils literal notranslate"><span class="pre">answer</span></code> and question properties are merged in from <code class="docutils literal notranslate"><span class="pre">df_questionnaire</span></code>. For each interview, only events are kept that precede the first occurrence of of any of the following events, <code class="docutils literal notranslate"><span class="pre">['RejectedBySupervisor',</span> <span class="pre">'OpenedBySupervisor',</span> <span class="pre">'OpenedByHQ',</span> <span class="pre">'RejectedByHQ']</span></code>. This limits the paradata, and all paradata based features to interviewing events, i.e. the interviewing that was done before the first interaction by the supervisor or HQ with the interview file.</p></li>
<li><p><strong>Append versions</strong>. The questionnaire, microdata and paradata dataframes are appended for all versions.</p></li>
</ol>
<p><strong>Indicator generation</strong></p>
<ol class="arabic simple" start="7">
<li><p><strong>Reduce to interviewing events</strong>. The paradata and microdata are reduced to data points created during <em>interviewing events</em> which is an approximation of the initial interview process, prior to any corrections or updates that may occur in an interview file after the first intervention by Supervisor or HQ roles. Interviewing events are identified as all events in the paradata for an interview file prior to the first event of the types: <code class="docutils literal notranslate"><span class="pre">['RejectedBySupervisor',</span> <span class="pre">'OpenedBySupervisor',</span> <span class="pre">'RejectedByHQ',</span> <span class="pre">'OpenedByHQ']</span></code>. For each interview, all subsequent events are removed from the pardata dataframe. The reduced paradata is then merged into the microdata and only those datapoints in the microdata are kept that have been produced during interviewing events.</p></li>
<li><p><strong>Construct features</strong>. Features are constructed using the reduced paradata or microdata. Features are built either on the unit level, i.e, the interview, or on the item level, i.e., the answer to a Survey Solutions question on a given roster instance/row. Features are absolute values, such as the duration of a question in seconds. <a class="reference internal" href="#FEATURES_SCORES.md"><span class="xref myst">FEATURES_SCORES.md</span></a> provides a description of how each indicator has been constructed.</p></li>
</ol>
<p><strong>Generating scores</strong></p>
<ol class="arabic simple" start="9">
<li><p>Individual features are evaluated and related scores calculated. While there is some variation, this is most commonly done by identifying anomalies within the feature on the item or unit level and then counting the share of anomalies on the unit level. In some cases the scores are built by normalizing features. For a detailed description of all scores, refer to <a class="reference internal" href="#FEATURES_SCORES.md"><span class="xref myst">FEATURES_SCORES.md</span></a>.</p></li>
<li><p>Individual scores are combined using Principal Component Analysis (PCA), a statistical technique to reduce the complexity of data (here the individual scores) while preserving the maximum amount of information. The result of the PCA is then normalized and <a class="reference external" href="https://en.wikipedia.org/wiki/Winsorizing">windsorized</a> to reduce extreme outliers. The resulting <code class="docutils literal notranslate"><span class="pre">unit_risk_score</span></code> ranges from 0 to 100.</p></li>
</ol>
<!-- what about the alternative methods, why did we decided against it, why PCS-->
<!-- can we weigh features? -->
</section>
<section id="roadmap">
<h1>Roadmap<a class="headerlink" href="#roadmap" title="Permalink to this heading"></a></h1>
<p>The following can be explored in the future to further increase the effectiveness and usability of RISSK.</p>
<ul class="simple">
<li><p><strong>Additional testing</strong>. While features, scores and algorithms have been thoroughly tested using the available experiment and testing data, further experiments could be conducted to explore how alternative feature design, scoring algorithms and clustering affect the overall score in other survey contexts.</p></li>
<li><p><strong>Expand/refine methodology</strong>.</p>
<ul>
<li><p>Identify new potential features e.g.:</p>
<ul>
<li><p>Count of QuestionDeclaredInvalid events in paradata once SurveySolution functionality becomes available.</p></li>
<li><p>Allow additional user input to specify broad survey parameters, e.g., specify cluster variable (to identify spacial and temporal anomalies)  or expected survey duration in days or sample size.</p></li>
<li><p>Sequence jumps to TimeStamps questions are more suspicious than for other questions</p></li>
<li><p>GPS questions recorded at different time than most of the interview.</p></li>
<li><p>Removing or changing the answers to gating questions (either linked or trigger enablement) may be indicative of interviewers trying to cut the length if an interview, especially at the beginning of a survey.</p></li>
</ul>
</li>
<li><p>Time series analysis of pauses.</p></li>
<li><p>Explore multi-variate anomaly detection</p></li>
<li><p>Identify different clusters of unwanted interview behaviour, e.g., fabricating interviewers may be different to interviewers who are struggling.</p></li>
</ul>
</li>
<li><p><strong>Feedback loop</strong>. Develop a standard framework for users to record the results of the review/verification and make it available as input for RISSK, so RISSK can learn from the review/verification during a survey.</p></li>
<li><p><strong>Facilitate ease of use</strong>:</p>
<ul>
<li><p>Wrapper functions can be written to facilitate easier workflows from other packages used to build survey pipelines, such as R or STATA.</p></li>
<li><p>Additional output file containing the scores and/or indicators</p></li>
<li><p>Output report or dashboard providing the user with additional information, e.g identification of the most influential scores, a dynamic summary of how URS develops over time, by interviewer/team</p></li>
</ul>
</li>
<li><p><strong>Obtain testing/training data</strong>. Additional testing data with interview-level quality labels would improve the validity and expose the package to a larger variety of survey settings, e.g. by:</p>
<ul>
<li><p>Obtaining data from a survey that used a systematic and thorough review/verification systems (e.g. random audio auditing).</p></li>
<li><p>Integrating RISSK into a survey quality system.</p></li>
<li><p>Produce fake data during training or post-survey.</p></li>
</ul>
</li>
<li><p><strong>Trained models</strong>. With additional training available, one can test alternative approaches of training models to identify at-risk interviews, using as inputs either constructed micro and paradata dataframes, the features or scores.</p></li>
<li><p><strong>Other CAPI tools</strong>. Expand to allow inputs from other CAPI tools.</p></li>
<li><p><strong>Server/API</strong>. Platform to receive feedback to learn, e.g. score tables and results of review/verification.</p></li>
<li><p><strong>Dissemination</strong>. Work can be done to raise awareness of the package among potential users and to stimulate the use, such as blog posts, presentations, courses, conference papers or supporting deployment among first users.</p></li>
</ul>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, Row Squared.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>