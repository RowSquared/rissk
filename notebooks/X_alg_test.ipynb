{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-18T13:12:07.884332Z",
     "start_time": "2023-08-18T13:12:07.751403Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "% load_ext autoreload\n",
    "% autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-18T13:12:07.971535Z",
     "start_time": "2023-08-18T13:12:07.918700Z"
    }
   },
   "outputs": [],
   "source": [
    "#%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-18T13:12:08.350334Z",
     "start_time": "2023-08-18T13:12:08.306178Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from hydra import initialize, compose\n",
    "\n",
    "from src.unit_proccessing import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Some helper functions\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_df_save_path(fc: UnitDataProcessing, fname: str):\n",
    "    return os.path.join(fc.config.data.raw, fc.config.surveys[0],\n",
    "                        fc.config.survey_version[0], 'processed_data', fname)\n",
    "\n",
    "\n",
    "def save_df(fc: UnitDataProcessing, df: pd.DataFrame, fname: str) -> None:\n",
    "    save_path = get_df_save_path(fc, fname)\n",
    "    pd.to_pickle(df, save_path)\n",
    "\n",
    "\n",
    "def load_df(fc: UnitDataProcessing, fname: str) -> pd.DataFrame:\n",
    "    load_path = get_df_save_path(fc, fname)\n",
    "    return pd.read_pickle(load_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-18T13:12:09.504083Z",
     "start_time": "2023-08-18T13:12:09.322595Z"
    }
   },
   "outputs": [],
   "source": [
    "with initialize(config_path='../configuration', version_base='1.1'):\n",
    "    config = compose(config_name='main.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-18T13:12:13.140093Z",
     "start_time": "2023-08-18T13:12:12.554172Z"
    }
   },
   "outputs": [],
   "source": [
    "features_class = UnitDataProcessing(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load data from disk. Execute notebook X_save_df.ipynb first to save data\n",
    "\"\"\"\n",
    "\n",
    "df_item = load_df(features_class, 'df_item.pkl')\n",
    "df_unit = load_df(features_class, 'df_unit.pkl')\n",
    "df_unit_score = load_df(features_class, 'df_unit_score.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Importing the main modue alg_test_utls\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import utils.alg_test_utils as alg_test_utils\n",
    "\n",
    "#reload(alg_test_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define the data for clustering. I prefer working with numpy arrays. Data is minmax scaled to 0,1 interval. For the moment all rows/columns with null values are thrown away\n",
    "\"\"\"\n",
    "\n",
    "df_for_clustering = alg_test_utils.drop_all_na(df_unit_score)\n",
    "keep_cols = [col for col in df_for_clustering.columns if col.startswith('s__')]\n",
    "data_for_clustering = df_for_clustering[keep_cols].values\n",
    "data_for_clustering = alg_test_utils.scale_data(data_for_clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define the parameters for testing\n",
    "\"\"\"\n",
    "\n",
    "ITERATIONS = 5\n",
    "TEST_SIZE = 0.15\n",
    "alg_test_utils.N_CLUSTERS = 2\n",
    "alg_test_utils.RANDOM_STATE = 321\n",
    "randgen = np.random.RandomState(alg_test_utils.RANDOM_STATE)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code checks the performance of algorithms against different subsets of data. Performance is measured using classical metrics for clustering algorithms. See source file and sklearn doc to learn more about the metrics used. Since different subsets of data are used at each iteration, we can only compare algorithms within the same iteration. Once the array of all pairs of scores is obtained, one should probably compute some mean value for each pair.\n",
    "\"\"\"\n",
    "\n",
    "train_data, _ = train_test_split(data_for_clustering, test_size=TEST_SIZE, random_state=randgen)\n",
    "\n",
    "res_labels = np.ndarray((ITERATIONS, len(alg_test_utils.CLUSTER_CLASSES), train_data.shape[0]), dtype=np.int32)\n",
    "\n",
    "# todo add random label assignment to compare scores\n",
    "\n",
    "res = []\n",
    "for i in range(ITERATIONS):\n",
    "    train_data, _ = train_test_split(data_for_clustering, test_size=TEST_SIZE, random_state=randgen)\n",
    "    d = dict(iteration=i, data=train_data, n_clusters=alg_test_utils.N_CLUSTERS,\n",
    "             random_state=alg_test_utils.RANDOM_STATE)\n",
    "\n",
    "    for j in range(len(alg_test_utils.CLUSTER_CLASSES)):\n",
    "        cluster_class = alg_test_utils.CLUSTER_CLASSES[j]\n",
    "\n",
    "        cls = alg_test_utils.fit_cluster(train_data, cluster_class)\n",
    "\n",
    "        labels = cls.labels_\n",
    "\n",
    "        d[f'{cluster_class.__name__}_labels'] = labels\n",
    "        res_labels[i, j] = labels\n",
    "\n",
    "    res.append(d)\n",
    "\n",
    "res_scores = np.ndarray((ITERATIONS, len(alg_test_utils.CLUSTER_CLASSES), len(alg_test_utils.CLUSTER_CLASSES),\n",
    "                         len(alg_test_utils.METRIC_SCORES)), dtype=np.float64)\n",
    "\n",
    "for i in range(res_scores.shape[0]):\n",
    "    for j1 in range(res_scores.shape[1]):\n",
    "        labels_a = res_labels[i, j1]\n",
    "        for j2 in range(res_scores.shape[3]):\n",
    "            labels_b = res_labels[i, j2]\n",
    "\n",
    "            pair_scores = alg_test_utils.calc_pair_metrics(labels_a, labels_b)\n",
    "\n",
    "            res_scores[i, j1, j2] = pair_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code checks the performance of algorithms against same subsets of data, but to which some transformation function is applied. Hence here comparisons between iterations make sense.\n",
    "\"\"\"\n",
    "\n",
    "res_labels = np.ndarray((ITERATIONS, len(alg_test_utils.CLUSTER_CLASSES), data_for_clustering.shape[0]), dtype=np.int32)\n",
    "\n",
    "# todo add random label assignment to compare scores\n",
    "\n",
    "res = []\n",
    "for i in range(ITERATIONS):\n",
    "    train_data = alg_test_utils.transform_data(data_for_clustering, random_state=randgen)\n",
    "    d = dict(iteration=i, data=train_data, n_clusters=alg_test_utils.N_CLUSTERS,\n",
    "             random_state=alg_test_utils.RANDOM_STATE)\n",
    "\n",
    "    for j in range(len(alg_test_utils.CLUSTER_CLASSES)):\n",
    "        cluster_class = alg_test_utils.CLUSTER_CLASSES[j]\n",
    "\n",
    "        cls = alg_test_utils.fit_cluster(train_data, cluster_class)\n",
    "\n",
    "        labels = cls.labels_\n",
    "\n",
    "        d[f'{cluster_class.__name__}_labels'] = labels\n",
    "        res_labels[i, j] = labels\n",
    "\n",
    "    res.append(d)\n",
    "\n",
    "res_scores = np.ndarray((ITERATIONS, len(alg_test_utils.CLUSTER_CLASSES), ITERATIONS,\n",
    "                         len(alg_test_utils.CLUSTER_CLASSES), len(alg_test_utils.METRIC_SCORES)), dtype=np.float64)\n",
    "\n",
    "for i1 in range(res_scores.shape[0]):\n",
    "    for j1 in range(res_scores.shape[1]):\n",
    "        labels_a = res_labels[i1, j1]\n",
    "        for i2 in range(res_scores.shape[2]):\n",
    "            for j2 in range(res_scores.shape[3]):\n",
    "                labels_b = res_labels[i2, j2]\n",
    "\n",
    "                pair_scores = alg_test_utils.calc_pair_metrics(labels_a, labels_b)\n",
    "\n",
    "                res_scores[i1, j1, i2, j2] = pair_scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
