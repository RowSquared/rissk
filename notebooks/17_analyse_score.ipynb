{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-31T11:45:15.509832Z",
     "start_time": "2023-08-31T11:45:15.481191Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-31T11:45:15.537827Z",
     "start_time": "2023-08-31T11:45:15.510399Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-31T11:45:16.038208Z",
     "start_time": "2023-08-31T11:45:15.538217Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils.import_utils import *\n",
    "from hydra import initialize, initialize_config_module, initialize_config_dir, compose\n",
    "from src.feature_processing import *\n",
    "from src.unit_proccessing import  *\n",
    "from src.utils.stats_utils import *\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-31T11:45:16.301882Z",
     "start_time": "2023-08-31T11:45:16.040573Z"
    }
   },
   "outputs": [],
   "source": [
    "with initialize(config_path='../configuration', version_base='1.1'):\n",
    "    config = compose(config_name='main.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-31T11:45:17.713454Z",
     "start_time": "2023-08-31T11:45:16.302422Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPORTING: EPforR_TAP with version EPforR_TAP_16. \n",
      "EPforR_TAP with version EPforR_TAP_16 loaded. \n",
      "Paradata shape: (1894, 49) Questionnaires shape: (188, 37) Microdata shape: (754, 42) \n",
      "IMPORTING: EPforR_TAP with version EPforR_TAP_13. \n",
      "EPforR_TAP with version EPforR_TAP_13 loaded. \n",
      "Paradata shape: (1978, 49) Questionnaires shape: (188, 37) Microdata shape: (785, 42) \n",
      "IMPORTING: EPforR_TAP with version EPforR_TAP_11. \n",
      "EPforR_TAP with version EPforR_TAP_11 loaded. \n",
      "Paradata shape: (5573, 49) Questionnaires shape: (180, 37) Microdata shape: (2023, 42) \n",
      "IMPORTING: EPforR_TAP with version EPforR_TAP_19. \n",
      "EPforR_TAP with version EPforR_TAP_19 loaded. \n",
      "Paradata shape: (2007, 49) Questionnaires shape: (188, 37) Microdata shape: (832, 42) \n",
      "IMPORTING: EPforR_TAP with version EPforR_TAP_15. \n",
      "EPforR_TAP with version EPforR_TAP_15 loaded. \n",
      "Paradata shape: (2264, 49) Questionnaires shape: (188, 37) Microdata shape: (883, 42) \n",
      "IMPORTING: EPforR_TAP with version EPforR_TAP_10. \n",
      "EPforR_TAP with version EPforR_TAP_10 loaded. \n",
      "Paradata shape: (447, 49) Questionnaires shape: (180, 37) Microdata shape: (157, 42) \n",
      "IMPORTING: EPforR_TAP with version EPforR_TAP_12. \n",
      "EPforR_TAP with version EPforR_TAP_12 loaded. \n",
      "Paradata shape: (58949, 49) Questionnaires shape: (180, 37) Microdata shape: (21527, 42) \n",
      "IMPORTING: EPforR_TAP with version EPforR_TAP_17. \n",
      "EPforR_TAP with version EPforR_TAP_17 loaded. \n",
      "Paradata shape: (2537, 49) Questionnaires shape: (188, 37) Microdata shape: (920, 42) \n",
      "IMPORTING: EPforR_TAP with version EPforR_TAP_14. \n",
      "EPforR_TAP with version EPforR_TAP_14 loaded. \n",
      "Paradata shape: (2313, 49) Questionnaires shape: (188, 37) Microdata shape: (778, 42) \n",
      "IMPORTING: EPforR_TAP with version EPforR_TAP_18. \n",
      "EPforR_TAP with version EPforR_TAP_18 loaded. \n",
      "Paradata shape: (1856, 49) Questionnaires shape: (188, 37) Microdata shape: (752, 42) \n",
      "Data Loaded\n",
      "Paradata Processed\n",
      "Items Build\n",
      "Unit Build\n"
     ]
    }
   ],
   "source": [
    "features_class = UnitDataProcessing(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-31T11:45:17.938236Z",
     "start_time": "2023-08-31T11:45:17.714080Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing f__answer_changed...\n",
      "Processing f__answer_position...\n",
      "Processing f__answer_selected...\n",
      "Processing f__first_decimal...\n",
      "Processing f__gps...\n",
      "ERROR ON FEATURE ITEM: f__gps, It won't be used in further calculation\n",
      "Processing f__numeric_response...\n"
     ]
    }
   ],
   "source": [
    "df_item = features_class.df_item\n",
    "self = features_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-31T11:45:17.988262Z",
     "start_time": "2023-08-31T11:45:17.938519Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing f__number_answered ...\n",
      "Processing f__number_unanswered ...\n"
     ]
    }
   ],
   "source": [
    "df_unit = features_class.df_unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-31T11:45:24.701420Z",
     "start_time": "2023-08-31T11:45:17.988708Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Score f__answer_changed...\n",
      "Processing f__gps...\n",
      "ERROR ON FEATURE ITEM: f__gps, It won't be used in further calculation\n",
      "Processing f__gps...\n",
      "ERROR ON FEATURE ITEM: f__gps, It won't be used in further calculation\n",
      "Processing Score f__answer_duration...\n",
      "Processing f__gps...\n",
      "ERROR ON FEATURE ITEM: f__gps, It won't be used in further calculation\n",
      "Processing f__gps...\n",
      "ERROR ON FEATURE ITEM: f__gps, It won't be used in further calculation\n",
      "Processing Score f__answer_hour_set...\n",
      "Processing f__gps...\n",
      "ERROR ON FEATURE ITEM: f__gps, It won't be used in further calculation\n",
      "Processing f__gps...\n",
      "ERROR ON FEATURE ITEM: f__gps, It won't be used in further calculation\n",
      "Processing Score f__answer_position...\n",
      "Processing f__gps...\n",
      "ERROR ON FEATURE ITEM: f__gps, It won't be used in further calculation\n",
      "Processing f__gps...\n",
      "ERROR ON FEATURE ITEM: f__gps, It won't be used in further calculation\n",
      "Processing Score f__answer_removed...\n",
      "Processing Score f__answer_selected...\n",
      "Processing f__gps...\n",
      "ERROR ON FEATURE ITEM: f__gps, It won't be used in further calculation\n",
      "Processing f__gps...\n",
      "ERROR ON FEATURE ITEM: f__gps, It won't be used in further calculation\n",
      "Processing Score f__first_decimal...\n",
      "Processing f__gps...\n",
      "ERROR ON FEATURE ITEM: f__gps, It won't be used in further calculation\n",
      "Processing f__gps...\n",
      "ERROR ON FEATURE ITEM: f__gps, It won't be used in further calculation\n",
      "Processing Score f__gps...\n",
      "Processing f__gps...\n",
      "ERROR ON FEATURE ITEM: f__gps, It won't be used in further calculation\n",
      "ERROR ON FEATURE SCORE: f__gps, It won't be used in further calculation\n",
      "Processing Score f__multi_option_question...\n",
      "Processing f__gps...\n",
      "ERROR ON FEATURE ITEM: f__gps, It won't be used in further calculation\n",
      "Processing f__gps...\n",
      "ERROR ON FEATURE ITEM: f__gps, It won't be used in further calculation\n",
      "Processing Score f__number_answered...\n",
      "Processing Score f__number_unanswered...\n",
      "Processing Score f__numeric_response...\n",
      "Processing Score f__pause_count...\n",
      "Processing Score f__pause_duration...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gabriele/App/rowsquared/mlss/src/item_processing.py:244: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[score_name] = 0\n",
      "/Users/gabriele/App/rowsquared/mlss/src/item_processing.py:271: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[score_name] = 0\n",
      "/Users/gabriele/App/rowsquared/mlss/src/item_processing.py:285: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[mask, score_name1] = 0\n",
      "/Users/gabriele/App/rowsquared/mlss/src/item_processing.py:286: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[mask, score_name2] = 0\n",
      "/Users/gabriele/App/rowsquared/mlss/src/item_processing.py:290: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.drop(columns=[score_name], inplace=True)\n",
      "/Users/gabriele/App/rowsquared/mlss/src/item_processing.py:280: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[mask, score_name] = model.predict(df[mask][[feature_name]])\n",
      "/Users/gabriele/App/rowsquared/mlss/src/item_processing.py:290: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.drop(columns=[score_name], inplace=True)\n",
      "/Users/gabriele/App/rowsquared/mlss/src/item_processing.py:280: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[mask, score_name] = model.predict(df[mask][[feature_name]])\n",
      "/Users/gabriele/App/rowsquared/mlss/src/item_processing.py:290: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.drop(columns=[score_name], inplace=True)\n",
      "/Users/gabriele/App/rowsquared/mlss/src/item_processing.py:280: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[mask, score_name] = model.predict(df[mask][[feature_name]])\n",
      "/Users/gabriele/App/rowsquared/mlss/src/item_processing.py:290: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.drop(columns=[score_name], inplace=True)\n",
      "/Users/gabriele/App/rowsquared/mlss/src/item_processing.py:375: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[score_name] = 0\n",
      "/Users/gabriele/App/rowsquared/mlss/src/item_processing.py:154: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[score_name] = 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Score f__sequence_jump...\n",
      "Processing f__gps...\n",
      "ERROR ON FEATURE ITEM: f__gps, It won't be used in further calculation\n",
      "Processing f__gps...\n",
      "ERROR ON FEATURE ITEM: f__gps, It won't be used in further calculation\n",
      "Processing Score f__time_changed...\n",
      "Processing Score f__total_duration...\n"
     ]
    }
   ],
   "source": [
    "df_unit_score = features_class.df_unit_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-23T16:09:38.981558Z",
     "start_time": "2023-08-23T16:09:28.542358Z"
    }
   },
   "outputs": [],
   "source": [
    "data = self.make_score__answer_duration()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-17T17:55:51.174103Z",
     "start_time": "2023-08-17T17:55:51.138945Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, AffinityPropagation, MeanShift\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "def clustering_comparison(df):\n",
    "    results = {}\n",
    "    \n",
    "    # K-Means\n",
    "    kmeans = KMeans(n_clusters=2)  # You might want to change the number of clusters based on domain knowledge\n",
    "    kmeans_labels = kmeans.fit_predict(df)\n",
    "    results['KMeans'] = silhouette_score(df, kmeans_labels)\n",
    "    \n",
    "    # DBSCAN\n",
    "    dbscan = DBSCAN()\n",
    "    dbscan_labels = dbscan.fit_predict(df)\n",
    "    # Compute silhouette score only if more than one cluster is identified\n",
    "    if len(np.unique(dbscan_labels)) > 1:\n",
    "        results['DBSCAN'] = silhouette_score(df, dbscan_labels)\n",
    "    \n",
    "    # Agglomerative Hierarchical Clustering\n",
    "    agglomerative = AgglomerativeClustering(n_clusters=2)  # Adjust number of clusters as needed\n",
    "    agg_labels = agglomerative.fit_predict(df)\n",
    "    results['Agglomerative'] = silhouette_score(df, agg_labels)\n",
    "    \n",
    "    # Affinity Propagation\n",
    "    affinity = AffinityPropagation()\n",
    "    affinity_labels = affinity.fit_predict(df)\n",
    "    results['AffinityPropagation'] = silhouette_score(df, affinity_labels)\n",
    "    \n",
    "    # Mean Shift\n",
    "    mean_shift = MeanShift()\n",
    "    mean_shift_labels = mean_shift.fit_predict(df)\n",
    "    # Compute silhouette score only if more than one cluster is identified\n",
    "    if len(np.unique(mean_shift_labels)) > 1:\n",
    "        results['MeanShift'] = silhouette_score(df, mean_shift_labels)\n",
    "    \n",
    "    # Gaussian Mixture Model (GMM)\n",
    "    gmm = GaussianMixture(n_components=2)  # Adjust number of components as needed\n",
    "    gmm_labels = gmm.fit_predict(df)\n",
    "    results['GMM'] = silhouette_score(df, gmm_labels)\n",
    "    \n",
    "    # Convert results to a DataFrame for better visualization\n",
    "    results_df = pd.DataFrame.from_dict(results, orient='index', columns=['Silhouette Score'])\n",
    "    \n",
    "    return results_df.sort_values(by='Silhouette Score', ascending=False)\n",
    "\n",
    "# Testing the function\n",
    "# df = df_unit_score[score_columns].copy()\n",
    "# #df = pd.DataFrame(scaler.fit_transform(df), columns=score_columns)\n",
    "# # df = pd.DataFrame(...)  # Your data here\n",
    "# df = df.fillna(0)\n",
    "# print(clustering_comparison(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-17T17:55:51.205243Z",
     "start_time": "2023-08-17T17:55:51.175453Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "def compare_clustering_algorithms(data):\n",
    "    # Scaling the data\n",
    "    scaler = StandardScaler()\n",
    "    data_scaled = scaler.fit_transform(data)\n",
    "    \n",
    "    # Defining the clustering algorithms\n",
    "    algorithms = {\n",
    "        'KMeans': KMeans(n_clusters=2), # You might want to find the optimal number of clusters first\n",
    "        'Agglomerative': AgglomerativeClustering(n_clusters=2),\n",
    "        'DBSCAN': DBSCAN(eps=0.5, min_samples=5),\n",
    "        'Isolation Forest': IsolationForest(contamination=0.2) # Contamination is the proportion of outliers in the data set\n",
    "    }\n",
    "    \n",
    "    # Applying the clustering algorithms and getting silhouette scores\n",
    "    silhouette_scores = {}\n",
    "    for name, algo in algorithms.items():\n",
    "        if name == \"Isolation Forest\":\n",
    "            # -1 for outliers, 1 for inliers -> transforming to 0 for inliers, 1 for outliers\n",
    "            labels = (algo.fit_predict(data_scaled) == -1).astype(int)\n",
    "        else:\n",
    "            labels = algo.fit_predict(data_scaled)\n",
    "            \n",
    "        # Calculating silhouette score (note: silhouette score is higher for better clusters)\n",
    "        score = silhouette_score(data_scaled, labels)\n",
    "        silhouette_scores[name] = score\n",
    "    \n",
    "    return silhouette_scores\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-17T17:55:51.238638Z",
     "start_time": "2023-08-17T17:55:51.208135Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "def clustering_stability(data, model, perturbation_factor=0.01, random_seed=None):\n",
    "    \"\"\"\n",
    "    Check the stability of a clustering model by applying perturbation to the data.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: The dataset to be clustered.\n",
    "    - model: A clustering model that has fit and predict methods, e.g., KMeans from sklearn.\n",
    "    - perturbation_factor: A small multiplier for the random noise added to the data.\n",
    "    - random_seed: Optional seed for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "    - ARI value: A value between -1 and 1. Values close to 1 indicate high stability.\n",
    "    \"\"\"\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    # Fit the model on the original data\n",
    "    original_labels = model.fit_predict(data)\n",
    "    \n",
    "    # Add small noise to the data\n",
    "    perturbed_data = data + perturbation_factor * np.random.randn(*data.shape)\n",
    "    \n",
    "    # Fit the model on the perturbed data\n",
    "    perturbed_labels = model.fit_predict(perturbed_data)\n",
    "    \n",
    "    # Compute Adjusted Rand Index to check stability\n",
    "    ari = adjusted_rand_score(original_labels, perturbed_labels)\n",
    "    \n",
    "    return ari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-17T17:56:13.975962Z",
     "start_time": "2023-08-17T17:55:51.239573Z"
    }
   },
   "outputs": [],
   "source": [
    "columns = ['s__answer_changed',\n",
    "       's__answer_duration_lower_outliers',\n",
    "       's__answer_duration__upper_outliers', 's__answer_position',\n",
    "       's__answer_removed', 's__answer_selected', 's__answer_hour_set',\n",
    "       's__first_decimal', 's__first_digit', 's__proximity_counts',\n",
    "       's__spatial_outlier', 's__gps', 's__multi_option_question',\n",
    "       's__number_answered', 's__number_unanswered', 's__pause_count',\n",
    "       's__pause_duration', 's__sequence_jump', 's__single_question',\n",
    "       's__time_changed', 's__total_duration', 's__total_elapse' ]\n",
    "\n",
    "\n",
    "# columns = ['s__answer_changed',\n",
    "#        's__answer_duration_lower_outliers',\n",
    "#        's__answer_duration__upper_outliers', \n",
    "#        's__answer_removed', 's__answer_selected', 's__answer_hour_set',\n",
    "#        's__first_decimal', 's__first_digit', 's__proximity_counts',\n",
    "#        's__spatial_outlier', 's__gps'\n",
    "#        , 's__number_unanswered', 's__pause_count',\n",
    "#        's__pause_duration', 's__sequence_jump', \n",
    "#        's__time_changed', 's__total_elapse' ]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df = self.df_unit_score[columns].copy()\n",
    "df = df.fillna(-1)\n",
    "df = pd.DataFrame(scaler.fit_transform(df), columns=columns)\n",
    "X = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-17T17:56:14.913890Z",
     "start_time": "2023-08-17T17:56:13.975434Z"
    }
   },
   "outputs": [],
   "source": [
    "#print(clustering_comparison(df))\n",
    "print(compare_clustering_algorithms(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-17T18:03:11.924764Z",
     "start_time": "2023-08-17T18:03:09.753721Z"
    }
   },
   "outputs": [],
   "source": [
    "for col in columns:\n",
    "    self.df_unit_score[col].hist()\n",
    "    plt.title(col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-17T17:56:25.314630Z",
     "start_time": "2023-08-17T17:56:23.521692Z"
    }
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2)\n",
    "stability_score = clustering_stability(X, kmeans,perturbation_factor=0.1)\n",
    "\n",
    "print(f\"Stability Score (ARI): {stability_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-17T17:56:25.487447Z",
     "start_time": "2023-08-17T17:56:25.332577Z"
    }
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2)  # You might want to change the number of clusters based on domain knowledge\n",
    "kmeans_labels = kmeans.fit_predict(X)\n",
    "df['score'] = kmeans_labels\n",
    "y = kmeans_labels\n",
    "print(df['score'].value_counts(), df['score'].value_counts()/df['score'].count(), silhouette_score(df, kmeans_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-17T17:56:26.681700Z",
     "start_time": "2023-08-17T17:56:25.485600Z"
    }
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-17T17:56:27.344108Z",
     "start_time": "2023-08-17T17:56:27.136285Z"
    }
   },
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(max_depth=6, random_state=0, n_estimators=10)\n",
    "rf.fit(X_train, Y_train)  \n",
    "print(rf.feature_importances_)\n",
    "importances = rf.feature_importances_\n",
    "indices = np.argsort(importances)\n",
    "features = X_train.columns\n",
    "plt.title('Feature Importance')\n",
    "plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write documentation on faking interview\n",
    "# The idea it would be to create a platform that compressed in anomised form the data with the feedback on the \"fraud\" to improve classification in the future\n",
    "\n",
    "# Take the case of one very long street household, how the Gps anomaly would act in the case?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlss",
   "language": "python",
   "name": "mlss"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
