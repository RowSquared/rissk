{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from hydra import initialize, initialize_config_module, initialize_config_dir, compose\n",
    "from src.item_processing import  *\n",
    "from src.utils.stats_utils import *\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with initialize(config_path='../configuration', version_base='1.1'):\n",
    "    config = compose(config_name='main.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_class = ItemFeatureProcessing(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Answer Single Question ITEM Level Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_item = features_class.df_item\n",
    "self = features_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_question_mask = ((self._df_item['type'] == 'SingleQuestion')\n",
    "                        & (self._df_item['n_answers'] > 2)\n",
    "                        & (self._df_item['is_filtered_combobox'] == False)\n",
    "                        & (pd.isnull(self._df_item['cascade_from_question_id'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_item[single_question_mask].groupby(['variable_name'])['value'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_name = 'f__single_question'\n",
    "score_name = 's__single_question'\n",
    "df = df_item[single_question_mask].copy()\n",
    "# Select only those variables that have at least three distinct values and more than one hundred records\n",
    "\n",
    "variables =  self.filter_variable_name_by_frequency(df, 'value', frequency=100, min_unique_values=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in variables:\n",
    "\n",
    "    mask = (df['variable_name']==var) \n",
    "    unique_values = df[mask]['value'].nunique()\n",
    "    entropy_df = df[mask].groupby('responsible')['value'].apply(calculate_entropy,unique_values=unique_values, min_record_sample=5).copy()\n",
    "    entropy_df = entropy_df.reset_index()\n",
    "    entropy_df = entropy_df[~pd.isnull(entropy_df['value'])]\n",
    "    \n",
    "    if entropy_df.shape[0] > 0:\n",
    "        entropy_df.sort_values('value', inplace=True, ascending=False)\n",
    "\n",
    "        median_value = entropy_df['value'].median()\n",
    "        entropy_df[score_name] = entropy_df['value'].apply(lambda x: 1 if x< median_value-50/100*median_value else 0)\n",
    "\n",
    "        anomaly_df = entropy_df[entropy_df[score_name] == 1]\n",
    "        no_anomaly_df = entropy_df[entropy_df[score_name] == 0]\n",
    "        plt.bar(no_anomaly_df['responsible'], no_anomaly_df['value'], color='blue', label='No Anomaly')\n",
    "        plt.bar(anomaly_df['responsible'], anomaly_df['value'], color='red', label='Anomaly')\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.title(var)\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in variables:\n",
    "    mask = (df['variable_name']==var)\n",
    "    unique_values = df[mask]['value'].nunique()\n",
    "    entropy_df = df[mask].groupby('responsible')['value'].apply(calculate_entropy,unique_values=unique_values)\n",
    "    entropy_df = entropy_df.reset_index()\n",
    "    entropy_df = entropy_df[~pd.isnull(entropy_df['value'])]\n",
    "    \n",
    "    if entropy_df.shape[0] > 0:\n",
    "        entropy_df.sort_values('value', inplace=True, ascending=False)\n",
    "\n",
    "        median_value = entropy_df['value'].median()\n",
    "\n",
    "        median_value = entropy_df['value'].median()\n",
    "        entropy_df[score_name] = entropy_df['value'].apply(lambda x: 1 if x < median_value - 50/100 * median_value else 0)\n",
    "        df.loc[mask,score_name] = df[mask]['responsible'].map(entropy_df.set_index('responsible')[score_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Answer Single Question Responsible Level Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.groupby(['responsible','variable_name']).agg({score_name: 'mean'})\n",
    "data = data.reset_index()\n",
    "entropy_ = data.groupby('responsible')[score_name].mean()\n",
    "entropy_.plot(kind='bar')\n",
    "plt.title('entropy__answer_position')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlss",
   "language": "python",
   "name": "mlss"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
